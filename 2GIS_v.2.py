import re
import json
import time
import base64
import logging
from pathlib import Path
from playwright.sync_api import TimeoutError as PlaywrightTimeoutError
from multiprocessing import Pool, cpu_count
from urllib.parse import unquote, urlparse
from urllib.parse import quote
from playwright.sync_api import sync_playwright


file_handler = logging.FileHandler("2GIS.log", mode="a", encoding="utf-8")
file_handler.setLevel(logging.INFO)  # –õ–æ–≥ –≤ —Ñ–∞–π–ª —Ç–æ–ª—å–∫–æ –≤–∞–∂–Ω–æ–µ

console_handler = logging.StreamHandler()
console_handler.setLevel(logging.DEBUG)  # –í –∫–æ–Ω—Å–æ–ª—å –≤—Å—ë (–µ—Å–ª–∏ –Ω—É–∂–Ω–æ)

logging.basicConfig(
    level=logging.DEBUG,
    format="%(asctime)s [%(levelname)s] %(message)s",
    datefmt="%Y-%m-%d %H:%M:%S",
    handlers=[file_handler, console_handler]
)

logger = logging.getLogger(__name__)

regions = [
    "–ê–º—É—Ä—Å–∫–∞—è –æ–±–ª–∞—Å—Ç—å",
    "–ê—Ä—Ö–∞–Ω–≥–µ–ª—å—Å–∫–∞—è –æ–±–ª–∞—Å—Ç—å",
    "–ê—Å—Ç—Ä–∞—Ö–∞–Ω—Å–∫–∞—è –æ–±–ª–∞—Å—Ç—å"
]

search_word = "–ê–≤—Ç–æ—Å–µ—Ä–≤–∏—Å"

args_list = [(region, search_word) for region in regions]
country = "ru"


def clean_invisible(text):
    return re.sub(r'\u2012|\u00a0|\u200b|\+7', '', text).strip()


def write_json_data(data: dict, filename: str | Path) -> None:
    path = Path(filename)

    # 1. –°–æ–∑–¥–∞—ë–º –≤—Å–µ –Ω–µ–¥–æ—Å—Ç–∞—é—â–∏–µ –∫–∞—Ç–∞–ª–æ–≥–∏
    path.parent.mkdir(parents=True, exist_ok=True)

    # 2. –ó–∞–ø–∏—Å—ã–≤–∞–µ–º —Ñ–∞–π–ª
    try:
        with path.open("w", encoding="utf-8") as f:
            json.dump(data, f, ensure_ascii=False, indent=4)
        logger.info(f"‚úÖ –î–∞–Ω–Ω—ã–µ —É—Å–ø–µ—à–Ω–æ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ {path}")
    except Exception as e:
        logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–∏ –¥–∞–Ω–Ω—ã—Ö –≤ {path}: {e}")


def read_json_data(filename):
    try:
        with open(filename, 'r', encoding='utf-8') as f:
            return json.load(f)
    except FileNotFoundError:
        return []
    except Exception as e:
        logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ —á—Ç–µ–Ω–∏–∏ –¥–∞–Ω–Ω—ã—Ö –∏–∑ {filename}: {e}")
        return []


def decode_possible_base64_url(url):
    """
    –†–∞—Å–ø–æ–∑–Ω–∞—ë—Ç base64-–∑–∞–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ —á–∞—Å—Ç–∏ –≤ —Å—Å—ã–ª–∫–µ –∏ –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç –ø–µ—Ä–≤—É—é —Å—Ç—Ä–æ–∫—É, –Ω–∞—á–∏–Ω–∞—é—â—É—é—Å—è —Å http.
    JSON –∏–≥–Ω–æ—Ä–∏—Ä—É–µ—Ç—Å—è.
    """
    try:
        for part in url.split('/'):
            # –ù–∞—Ö–æ–¥–∏–º base64-–ø–æ–¥–æ–±–Ω—ã–µ —Ñ—Ä–∞–≥–º–µ–Ω—Ç—ã
            if part.startswith(('aHR0', 'ey')) and len(part) >= 16:
                padded = part + '=' * (-len(part) % 4)
                try:
                    decoded = base64.urlsafe_b64decode(padded).decode(
                        'utf-8', errors='replace').strip()
                except Exception:
                    continue

                # –û–±—Ä–∞–±–æ—Ç–∫–∞ \n –∏ –º–Ω–æ–≥–æ—Å—Ç—Ä–æ—á–Ω—ã—Ö base64
                decoded = decoded.replace('\\n', '\n')
                for line in decoded.splitlines():
                    if line.strip().startswith("http"):
                        return line.strip()

                # –ï—Å–ª–∏ —Å—Å—ã–ª–∫–∞ –Ω–µ –Ω–∞–π–¥–µ–Ω–∞, –º–æ–∂–Ω–æ –≤–µ—Ä–Ω—É—Ç—å –≤–µ—Å—å –¥–µ–∫–æ–¥
                return decoded
    except Exception:
        pass

    # –ï—Å–ª–∏ –Ω–∏—á–µ–≥–æ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ ‚Äî –≤–µ—Ä–Ω—É—Ç—å –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã–π url
    return url


def get_header(wrapper):
    name = wrapper.locator("h1").text_content()

    try:
        rating = wrapper.locator("._y10azs").inner_text(timeout=300)
    except:
        rating = ""

    reviews_elem = wrapper.locator("._jspzdm")
    count_reviews = reviews_elem.text_content(
    ).split()[0] if reviews_elem.count() > 0 else ""

    return name, rating, count_reviews


def get_data_card(wrapper):
    address = None
    website = None
    email = None
    phones = []

    # –ê–¥—Ä–µ—Å
    selectors = [
        "div._172gbf8 >> ._49kxlr >> ._13eh3hvq >> ._oqoid",               # —Ä–µ–∑–µ—Ä–≤
        "div._172gbf8 >> ._49kxlr >> ._oqoid",                             # –≤—Ç–æ—Ä–æ–π —Ä–µ–∑–µ—Ä–≤
        "div._172gbf8 >> ._49kxlr >> ._13eh3hvq >> ._14quei >> ._wrdavn"
    ]

    for sel in selectors:
        locator = wrapper.locator(sel)
        if locator.count() > 0:
            try:
                address = locator.nth(0).inner_text(timeout=500).strip()
                break
            except Exception as e:
                logger.debug(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ —á—Ç–µ–Ω–∏–∏ –∞–¥—Ä–µ—Å–∞ –∏–∑ {sel}: {e}")
    if not address:
        logger.debug("‚ùå –ê–¥—Ä–µ—Å –Ω–µ –Ω–∞–π–¥–µ–Ω –Ω–∏ –ø–æ –æ–¥–Ω–æ–º—É –∏–∑ —Å–µ–ª–µ–∫—Ç–æ—Ä–æ–≤")

    # Email
    try:
        email_info = wrapper.locator(
            'div._172gbf8 >> ._49kxlr >> div >> a[href^="mailto:"]')
        if email_info.count() > 0:
            email = email_info.first.inner_text().strip()
    except Exception as e:
        logger.debug(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–ª—É—á–µ–Ω–∏–∏ email: {e}")

    # Website
    try:
        website_info = wrapper.locator(
            'div._172gbf8 >> ._49kxlr >> div >> a[href^="https://"]')
        if website_info.count() > 0:
            website = website_info.first.inner_text().strip()
    except Exception as e:
        logger.debug(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–ª—É—á–µ–Ω–∏–∏ —Å–∞–π—Ç–∞: {e}")

    # –ö–Ω–æ–ø–∫–∞ "–ü–æ–∫–∞–∑–∞—Ç—å –≤—Å–µ —Ç–µ–ª–µ—Ñ–æ–Ω—ã"
    try:
        view_all_phones = wrapper.locator("._1tkj2hw")
        if view_all_phones.count() > 0:
            view_all_phones.first.click()
    except Exception as e:
        logger.debug(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –∫–ª–∏–∫–µ –Ω–∞ –∫–Ω–æ–ø–∫—É —Ç–µ–ª–µ—Ñ–æ–Ω–æ–≤: {e}")

    # –¢–µ–ª–µ—Ñ–æ–Ω—ã (—É–º–Ω–æ–µ –æ–∂–∏–¥–∞–Ω–∏–µ –ø—Ä–æ–≥—Ä—É–∑–∫–∏)
    try:
        phones = []
        max_attempts = 5
        attempt = 0

        while attempt < max_attempts:
            phones_info = wrapper.locator(
                'div._172gbf8 >> ._49kxlr >> div >> a[href^="tel:"]')
            raw_phones = []

            for i in range(phones_info.count()):
                phone = phones_info.nth(i).inner_text().strip()
                raw_phones.append(phone)

            if all("..." not in p for p in raw_phones):
                phones = raw_phones
                break

            logger.warning(f"‚ö† –ù–∞–π–¥–µ–Ω—ã –Ω–æ–º–µ—Ä–∞ —Å –º–Ω–æ–≥–æ—Ç–æ—á–∏–µ–º: {raw_phones}")
            if view_all_phones.count() > 0:
                logger.debug(
                    f"üîÅ –ü–æ–ø—ã—Ç–∫–∞ #{attempt + 1} ‚Äî –∫–ª–∏–∫–∞–µ–º –ø–æ–≤—Ç–æ—Ä–Ω–æ –ø–æ –∫–Ω–æ–ø–∫–µ —Ç–µ–ª–µ—Ñ–æ–Ω–æ–≤")
                view_all_phones.first.click()

            attempt += 1

        # –¥–∞–∂–µ –µ—Å–ª–∏ –µ—Å—Ç—å "...", —Å–æ—Ö—Ä–∞–Ω—è–µ–º –≤—Å—ë —á—Ç–æ –Ω–∞—à–ª–∏
        if not phones:
            phones = raw_phones

    except Exception as e:
        logger.debug(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–ª—É—á–µ–Ω–∏–∏ —Ç–µ–ª–µ—Ñ–æ–Ω–æ–≤: {e}")

    return address, email, phones, website


def get_socials(wrapper):
    data = {
        'Instagram': [],
        'Telegram': [],
        'WhatsApp': [],
        '–í–ö–æ–Ω—Ç–∞–∫—Ç–µ': [],
        '–û–¥–Ω–æ–∫–ª–∞—Å—Å–Ω–∏–∫–∏': [],
        'Facebook': [],
        'Twitter': [],
        'YouTube': []
    }

    links = wrapper.locator(
        '[aria-label="Instagram"], [aria-label="Telegram"], [aria-label="WhatsApp"],'
        '[aria-label="–í–ö–æ–Ω—Ç–∞–∫—Ç–µ"], [aria-label="–û–¥–Ω–æ–∫–ª–∞—Å—Å–Ω–∏–∫–∏"],'
        '[aria-label="Facebook"], [aria-label="Twitter"], [aria-label="YouTube"]'
    )
    count = links.count()

    for i in range(count):
        link = links.nth(i)
        raw_url = link.get_attribute("href")
        name = link.get_attribute("aria-label")

        if raw_url:
            decoded_url = unquote(raw_url)
            final_data = decode_possible_base64_url(decoded_url)

            if name == "WhatsApp":
                match = re.search(r'wa\.me/(\d+)', final_data)
                value = match.group(1) if match else final_data

            elif name == "Telegram":
                match = re.search(r't\.me/([\w@+]+)', final_data)
                value = f"@{match.group(1)}" if match else final_data

            elif name == "–í–ö–æ–Ω—Ç–∞–∫—Ç–µ":
                match = re.search(r'vk\.com/([\w-]+)', final_data)
                value = match.group(1) if match else final_data

            elif name == "–û–¥–Ω–æ–∫–ª–∞—Å—Å–Ω–∏–∫–∏":
                match = re.search(r'ok\.ru/([\w-]+)', final_data)
                value = match.group(1) if match else final_data

            elif name == "YouTube":
                match = re.search(r'youtube\.com/(@[\w\d_-]+)', final_data)
                value = match.group(1) if match else final_data

            elif name == "Twitter":
                match = re.search(r'twitter\.com/([\w-]+)', final_data)
                value = match.group(1) if match else final_data

            else:
                try:
                    parsed = urlparse(final_data)
                    value = parsed.netloc + parsed.path
                    value = value.replace("www.", "").lstrip('/')
                except Exception:
                    value = final_data

            data[name].append(value.strip())

    return data


def get_pagination_info(page, selector="div._jcreqo >> ._1xhlznaa", max_cards=12):
    try:
        count_cards_text = page.locator(selector).inner_text(timeout=500)
        count_cards = int(count_cards_text)
    except:
        count_cards = 0
    count_pages = count_cards // max_cards
    last_page_count_cards = count_cards % max_cards
    return count_cards, count_pages, last_page_count_cards


def run_parser_for_region(region, search_word):
    old_data = read_json_data(f"output/{region}/{search_word}.json")
    existing_names = set(x["name"] for x in old_data)
    collect_data = []

    def process_data(existing_data, new_card_data):
        if new_card_data["name"] not in (x["name"] for x in existing_data):
            collect_data.append(new_card_data)
            existing_data.append(new_card_data)
    try:

        with sync_playwright() as p:
            start_time_program = time.time()
            browser = p.chromium.launch(
                headless=True,
                args=["--disable-blink-features=AutomationControlled"],
            )
            context = browser.new_context(
                user_agent="Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/124.0.0.0 Safari/537.36",
                permissions=[],
                viewport={"width": 1280, "height": 800},
                java_script_enabled=True,
                record_video_dir=None,
                bypass_csp=True,  # –í–æ–∑–º–æ–∂–Ω–æ —É–±—Ä–∞—Ç—å
            )
            context.set_default_timeout(1500)
            page = context.new_page()
            page.set_default_timeout(3000)

            page.goto(
                f"https://2gis.{country}/search/{quote(region)}%20{quote(search_word)}",
                wait_until="domcontentloaded",
                timeout=5000
            )

            count_cards, count_pages, last_page_count_cards = get_pagination_info(
                page)

            for page_index in range(count_pages + 1):
                start_time_page = time.time()
                logger.info(
                    f"–ü–µ—Ä–µ—Ö–æ–¥ –ø–æ —Å—Ç—Ä–∞–Ω–∏—Ü–µ {page_index + 1} –∏–∑ {count_pages + 1}")
                items = page.locator("._1kf6gff")
                count = items.count()

                for i in range(min(12, count if page_index != count_pages else last_page_count_cards)):
                    try:
                        start_time_card = time.time()
                        item = items.nth(i)
                        if not item.is_visible():
                            continue
                        item.click()
                        page.wait_for_selector("._fjltwx h1", timeout=1500)
                        wrapper = page.locator("._fjltwx")

                        preview_name = wrapper.locator(
                            "h1").text_content().strip()

                        if preview_name in existing_names:
                            logger.debug(
                                f"[{i + 1}] ‚è© {preview_name} —É–∂–µ –≤ –±–∞–∑–µ, –ø—Ä–æ–ø—É—Å–∫")
                            continue
                        else:
                            existing_names.add(preview_name)

                        if wrapper.count() == 0:
                            logger.warning(
                                f"[{i + 1}] ‚ùå –ö–∞—Ä—Ç–æ—á–∫–∞ –Ω–µ –∑–∞–≥—Ä—É–∑–∏–ª–∞—Å—å")
                            continue

                        name, rating, count_reviews = get_header(wrapper)
                        address, email, phones, website = get_data_card(
                            wrapper)
                        socials = get_socials(wrapper)

                        close_button = page.locator('div._k1uvy >> svg')
                        if close_button.count() > 0:
                            close_button.nth(0).click()

                        logger.info(
                            f"[{i + 1}] ‚úÖ {name} –£—Å–ø–µ—à–Ω–æ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–∞ –∑–∞ {round(time.time() - start_time_card, 2)} —Å–µ–∫")

                        data_card = {
                            "name": clean_invisible(name),
                            "rating": clean_invisible(rating),
                            "count_reviews": clean_invisible(count_reviews),
                            "address": clean_invisible(address) if address else None,
                            "email": clean_invisible(email) if email else None,
                            "phones": [clean_invisible(phone) for phone in phones] if phones else None,
                            "website": clean_invisible(website) if website else None,
                            "socials": {k: v for k, v in socials.items()}
                        }

                        process_data(old_data, data_card)
                    except Exception as e:
                        logger.exception(
                            f"[{i + 1}] ‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ –∫–∞—Ä—Ç–æ—á–∫–∏")

                logger.info(
                    f"üìÑ –°—Ç—Ä–∞–Ω–∏—Ü–∞ {page_index + 1}, —Å–æ–±—Ä–∞–Ω–æ: {len(collect_data)}, –≤—Ä–µ–º—è: {round(time.time() - start_time_page, 2)} —Å–µ–∫")

                if page_index != count_pages:
                    next_buttons = page.locator(
                        'div._1x4k6z7 >> ._n5hmn94 >> svg')
                    if next_buttons.count() > 0:
                        try:
                            next_buttons.nth(1 if page_index >
                                             0 else 0).click()
                            page.wait_for_selector("._1kf6gff", timeout=1500)
                        except Exception as e:
                            logger.warning(f"–ù–µ —É–¥–∞–ª–æ—Å—å –Ω–∞–∂–∞—Ç—å '–¥–∞–ª—å—à–µ': {e}")
                else:
                    logger.info("‚úÖ –ü–æ—Å–ª–µ–¥–Ω—è—è —Å—Ç—Ä–∞–Ω–∏—Ü–∞ –¥–æ—Å—Ç–∏–≥–Ω—É—Ç–∞")

    except Exception as e:
        logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–ø—É—Å–∫–µ –±—Ä–∞—É–∑–µ—Ä–∞: {e}")
    finally:
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ç–æ–ª—å–∫–æ –æ–¥–∏–Ω —Ä–∞–∑
        write_json_data(
            old_data, Path("output") / region / f"{search_word}.json")
        logger.info("–ü—Ä–æ–≥—Ä–∞–º–º–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∞")
        logger.info(
            f"–í—Å–µ–≥–æ —Å–æ–±—Ä–∞–Ω–æ: {len(collect_data)}, –≤—Ä–µ–º—è —Ä–∞–±–æ—Ç—ã: {round(time.time() - start_time_program, 2)} —Å–µ–∫")


if __name__ == '__main__':
    try:
        with Pool(processes=3) as pool:
            pool.starmap(run_parser_for_region, args_list)

            print("‚úÖ –í—Å–µ –ø—Ä–æ—Ü–µ—Å—Å—ã –∑–∞–≤–µ—Ä—à–µ–Ω—ã")
    except KeyboardInterrupt:
        print("‚õî –ü—Ä–µ—Ä–≤–∞–Ω–æ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–º. –ü—Ä–æ—Ü–µ—Å—Å—ã –æ—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω—ã.")
