import re
import json
import time
import base64
import logging
from pathlib import Path
from playwright.sync_api import TimeoutError as PlaywrightTimeoutError
from multiprocessing import Pool, cpu_count
from urllib.parse import unquote, urlparse
from urllib.parse import quote
from playwright.sync_api import sync_playwright


file_handler = logging.FileHandler("2GIS.log", mode="a", encoding="utf-8")
file_handler.setLevel(logging.INFO)  # –õ–æ–≥ –≤ —Ñ–∞–π–ª —Ç–æ–ª—å–∫–æ –≤–∞–∂–Ω–æ–µ

console_handler = logging.StreamHandler()
console_handler.setLevel(logging.DEBUG)  # –í –∫–æ–Ω—Å–æ–ª—å –≤—Å—ë (–µ—Å–ª–∏ –Ω—É–∂–Ω–æ)

logging.basicConfig(
    level=logging.DEBUG,
    format="%(asctime)s [%(levelname)s] %(message)s",
    datefmt="%Y-%m-%d %H:%M:%S",
    handlers=[file_handler, console_handler]
)

logger = logging.getLogger(__name__)

regions = [
    # "–ê–º—É—Ä—Å–∫–∞—è –æ–±–ª–∞—Å—Ç—å",
    # "–ê—Ä—Ö–∞–Ω–≥–µ–ª—å—Å–∫–∞—è –æ–±–ª–∞—Å—Ç—å",
    # "–ê—Å—Ç—Ä–∞—Ö–∞–Ω—Å–∫–∞—è –æ–±–ª–∞—Å—Ç—å",
    # "–ë–µ–ª–≥–æ—Ä–æ–¥—Å–∫–∞—è –æ–±–ª–∞—Å—Ç—å",
    # "–ë—Ä—è–Ω—Å–∫–∞—è –æ–±–ª–∞—Å—Ç—å",
    # "–í–ª–∞–¥–∏–º–∏—Ä—Å–∫–∞—è –æ–±–ª–∞—Å—Ç—å",
    # "–í–æ–ª–≥–æ–≥—Ä–∞–¥—Å–∫–∞—è –æ–±–ª–∞—Å—Ç—å",
    # "–í–æ–ª–æ–≥–æ–¥—Å–∫–∞—è –æ–±–ª–∞—Å—Ç—å",
    # "–í–æ—Ä–æ–Ω–µ–∂—Å–∫–∞—è –æ–±–ª–∞—Å—Ç—å",
    # "–ò–≤–∞–Ω–æ–≤—Å–∫–∞—è –æ–±–ª–∞—Å—Ç—å",
    # "–ò—Ä–∫—É—Ç—Å–∫–∞—è –æ–±–ª–∞—Å—Ç—å",
    # "–ö–∞–ª–∏–Ω–∏–Ω–≥—Ä–∞–¥—Å–∫–∞—è –æ–±–ª–∞—Å—Ç—å",
    # "–ö–∞–ª—É–∂—Å–∫–∞—è –æ–±–ª–∞—Å—Ç—å",
    # "–ö–∞–º—á–∞—Ç—Å–∫–∞—è –æ–±–ª–∞—Å—Ç—å",
    # "–ö–µ–º–µ—Ä–æ–≤—Å–∫–∞—è –æ–±–ª–∞—Å—Ç—å",
    # "–ö–∏—Ä–æ–≤—Å–∫–∞—è –æ–±–ª–∞—Å—Ç—å",
    # "–ö–æ—Å—Ç—Ä–æ–º—Å–∫–∞—è –æ–±–ª–∞—Å—Ç—å",
    # "–ö—É—Ä–≥–∞–Ω—Å–∫–∞—è –æ–±–ª–∞—Å—Ç—å",
    # "–ö—É—Ä—Å–∫–∞—è –æ–±–ª–∞—Å—Ç—å",
    # "–õ–µ–Ω–∏–Ω–≥—Ä–∞–¥—Å–∫–∞—è –æ–±–ª–∞—Å—Ç—å",
    # "–õ–∏–ø–µ—Ü–∫–∞—è –æ–±–ª–∞—Å—Ç—å",
    "–ú–∞–≥–∞–¥–∞–Ω—Å–∫–∞—è –æ–±–ª–∞—Å—Ç—å",
    # "–ú–æ—Å–∫–æ–≤—Å–∫–∞—è –æ–±–ª–∞—Å—Ç—å",
    # "–ú—É—Ä–º–∞–Ω—Å–∫–∞—è –æ–±–ª–∞—Å—Ç—å",
    # "–ù–∏–∂–µ–≥–æ—Ä–æ–¥—Å–∫–∞—è –æ–±–ª–∞—Å—Ç—å",
    # "–ù–æ–≤–≥–æ—Ä–æ–¥—Å–∫–∞—è –æ–±–ª–∞—Å—Ç—å",
    # "–ù–æ–≤–æ—Å–∏–±–∏—Ä—Å–∫–∞—è –æ–±–ª–∞—Å—Ç—å",
    # "–û–º—Å–∫–∞—è –æ–±–ª–∞—Å—Ç—å",
    # "–û—Ä–µ–Ω–±—É—Ä–≥—Å–∫–∞—è –æ–±–ª–∞—Å—Ç—å",
    # "–û—Ä–ª–æ–≤—Å–∫–∞—è –æ–±–ª–∞—Å—Ç—å",
    # "–ü–µ–Ω–∑–µ–Ω—Å–∫–∞—è –æ–±–ª–∞—Å—Ç—å",
    # "–ü–µ—Ä–º—Å–∫–∞—è –æ–±–ª–∞—Å—Ç—å",
    # "–ü—Å–∫–æ–≤—Å–∫–∞—è –æ–±–ª–∞—Å—Ç—å",
    # "–†–æ—Å—Ç–æ–≤—Å–∫–∞—è –æ–±–ª–∞—Å—Ç—å",
    # "–†—è–∑–∞–Ω—Å–∫–∞—è –æ–±–ª–∞—Å—Ç—å",
    # "–°–∞–º–∞—Ä—Å–∫–∞—è –æ–±–ª–∞—Å—Ç—å",
    # "–°–∞—Ä–∞—Ç–æ–≤—Å–∫–∞—è –æ–±–ª–∞—Å—Ç—å",
    # "–°–∞—Ö–∞–ª–∏–Ω—Å–∫–∞—è –æ–±–ª–∞—Å—Ç—å",
    # "–°–≤–µ—Ä–¥–ª–æ–≤—Å–∫–∞—è –æ–±–ª–∞—Å—Ç—å",
    # "–°–º–æ–ª–µ–Ω—Å–∫–∞—è –æ–±–ª–∞—Å—Ç—å",
    # "–¢–∞–º–±–æ–≤—Å–∫–∞—è –æ–±–ª–∞—Å—Ç—å",
    # "–¢–≤–µ—Ä—Å–∫–∞—è –æ–±–ª–∞—Å—Ç—å",
    # "–¢–æ–º—Å–∫–∞—è –æ–±–ª–∞—Å—Ç—å",
    # "–¢—É–ª—å—Å–∫–∞—è –æ–±–ª–∞—Å—Ç—å",
    # "–¢—é–º–µ–Ω—Å–∫–∞—è –æ–±–ª–∞—Å—Ç—å",
    # "–£–ª—å—è–Ω–æ–≤—Å–∫–∞—è –æ–±–ª–∞—Å—Ç—å",
    # "–ß–µ–ª—è–±–∏–Ω—Å–∫–∞—è –æ–±–ª–∞—Å—Ç—å",
    # "–Ø—Ä–æ—Å–ª–∞–≤—Å–∫–∞—è –æ–±–ª–∞—Å—Ç—å"
]

search_words = [
    "–ê–≤—Ç–æ—Å–µ—Ä–≤–∏—Å",
    # "–ö—Ä–∞—Å–æ—Ç–∞",
]

args_list = [(region, word) for word in search_words for region in regions]
country = "ru"


def clean_invisible(text):
    return re.sub(r'\u2012|\u00a0|\u200b|\+7', '', text).strip()


def write_json_data(data: dict, filename: str | Path) -> None:
    path = Path(filename)

    # 1. –°–æ–∑–¥–∞—ë–º –≤—Å–µ –Ω–µ–¥–æ—Å—Ç–∞—é—â–∏–µ –∫–∞—Ç–∞–ª–æ–≥–∏
    path.parent.mkdir(parents=True, exist_ok=True)

    # 2. –ó–∞–ø–∏—Å—ã–≤–∞–µ–º —Ñ–∞–π–ª
    try:
        with path.open("w", encoding="utf-8") as f:
            json.dump(data, f, ensure_ascii=False, indent=4)
        logger.info(f"‚úÖ –î–∞–Ω–Ω—ã–µ —É—Å–ø–µ—à–Ω–æ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ {path}")
    except Exception as e:
        logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–∏ –¥–∞–Ω–Ω—ã—Ö –≤ {path}: {e}")


def read_json_data(filename):
    try:
        with open(filename, 'r', encoding='utf-8') as f:
            return json.load(f)
    except FileNotFoundError:
        return []
    except Exception as e:
        logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ —á—Ç–µ–Ω–∏–∏ –¥–∞–Ω–Ω—ã—Ö –∏–∑ {filename}: {e}")
        return []


def decode_possible_base64_url(url):
    """
    –†–∞—Å–ø–æ–∑–Ω–∞—ë—Ç base64-–∑–∞–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ —á–∞—Å—Ç–∏ –≤ —Å—Å—ã–ª–∫–µ –∏ –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç –ø–µ—Ä–≤—É—é —Å—Ç—Ä–æ–∫—É, –Ω–∞—á–∏–Ω–∞—é—â—É—é—Å—è —Å http.
    JSON –∏–≥–Ω–æ—Ä–∏—Ä—É–µ—Ç—Å—è.
    """
    try:
        for part in url.split('/'):
            # –ù–∞—Ö–æ–¥–∏–º base64-–ø–æ–¥–æ–±–Ω—ã–µ —Ñ—Ä–∞–≥–º–µ–Ω—Ç—ã
            if part.startswith(('aHR0', 'ey')) and len(part) >= 16:
                padded = part + '=' * (-len(part) % 4)
                try:
                    decoded = base64.urlsafe_b64decode(padded).decode(
                        'utf-8', errors='replace').strip()
                except Exception:
                    continue

                # –û–±—Ä–∞–±–æ—Ç–∫–∞ \n –∏ –º–Ω–æ–≥–æ—Å—Ç—Ä–æ—á–Ω—ã—Ö base64
                decoded = decoded.replace('\\n', '\n')
                for line in decoded.splitlines():
                    if line.strip().startswith("http"):
                        return line.strip()

                # –ï—Å–ª–∏ —Å—Å—ã–ª–∫–∞ –Ω–µ –Ω–∞–π–¥–µ–Ω–∞, –º–æ–∂–Ω–æ –≤–µ—Ä–Ω—É—Ç—å –≤–µ—Å—å –¥–µ–∫–æ–¥
                return decoded
    except Exception:
        pass

    # –ï—Å–ª–∏ –Ω–∏—á–µ–≥–æ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ ‚Äî –≤–µ—Ä–Ω—É—Ç—å –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã–π url
    return url


def get_header(wrapper):
    name = wrapper.locator("h1").text_content()

    try:
        rating = wrapper.locator("._y10azs").inner_text(timeout=300)
    except:
        rating = ""

    reviews_elem = wrapper.locator("._jspzdm")
    count_reviews = reviews_elem.text_content(
    ).split()[0] if reviews_elem.count() > 0 else ""

    return name, rating, count_reviews


def get_data_card(wrapper):
    address = None
    website = None
    email = None
    phones = []

    # –ê–¥—Ä–µ—Å
    selectors = [
        "div._172gbf8 >> ._49kxlr >> ._13eh3hvq >> ._oqoid",               # —Ä–µ–∑–µ—Ä–≤
        "div._172gbf8 >> ._49kxlr >> ._oqoid",                             # –≤—Ç–æ—Ä–æ–π —Ä–µ–∑–µ—Ä–≤
        "div._172gbf8 >> ._49kxlr >> ._13eh3hvq >> ._14quei >> ._wrdavn"
    ]

    for sel in selectors:
        locator = wrapper.locator(sel)
        if locator.count() > 0:
            try:
                address = locator.nth(0).inner_text(timeout=500).strip()
                break
            except Exception as e:
                logger.debug(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ —á—Ç–µ–Ω–∏–∏ –∞–¥—Ä–µ—Å–∞ –∏–∑ {sel}: {e}")
    if not address:
        logger.debug("‚ùå –ê–¥—Ä–µ—Å –Ω–µ –Ω–∞–π–¥–µ–Ω –Ω–∏ –ø–æ –æ–¥–Ω–æ–º—É –∏–∑ —Å–µ–ª–µ–∫—Ç–æ—Ä–æ–≤")

    # Email
    try:
        email_info = wrapper.locator(
            'div._172gbf8 >> ._49kxlr >> div >> a[href^="mailto:"]')
        if email_info.count() > 0:
            email = email_info.first.inner_text().strip()
    except Exception as e:
        logger.debug(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–ª—É—á–µ–Ω–∏–∏ email: {e}")

    # Website
    try:
        website_info = wrapper.locator(
            'div._172gbf8 >> ._49kxlr >> div >> a[href^="https://"]')
        if website_info.count() > 0:
            website = website_info.first.inner_text().strip()
    except Exception as e:
        logger.debug(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–ª—É—á–µ–Ω–∏–∏ —Å–∞–π—Ç–∞: {e}")

    # –ö–Ω–æ–ø–∫–∞ "–ü–æ–∫–∞–∑–∞—Ç—å –≤—Å–µ —Ç–µ–ª–µ—Ñ–æ–Ω—ã"
    try:
        view_all_phones = wrapper.locator("._1tkj2hw")
        if view_all_phones.count() > 0:
            view_all_phones.first.click()
    except Exception as e:
        logger.debug(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –∫–ª–∏–∫–µ –Ω–∞ –∫–Ω–æ–ø–∫—É —Ç–µ–ª–µ—Ñ–æ–Ω–æ–≤: {e}")

    # –¢–µ–ª–µ—Ñ–æ–Ω—ã (—É–º–Ω–æ–µ –æ–∂–∏–¥–∞–Ω–∏–µ –ø—Ä–æ–≥—Ä—É–∑–∫–∏)
    try:
        phones = []
        max_attempts = 5
        attempt = 0

        while attempt < max_attempts:
            phones_info = wrapper.locator(
                'div._172gbf8 >> ._49kxlr >> div >> a[href^="tel:"]')
            raw_phones = []

            for i in range(phones_info.count()):
                phone = phones_info.nth(i).inner_text().strip()
                raw_phones.append(phone)

            if all("..." not in p for p in raw_phones):
                phones = raw_phones
                break

            logger.warning(f"‚ö† –ù–∞–π–¥–µ–Ω—ã –Ω–æ–º–µ—Ä–∞ —Å –º–Ω–æ–≥–æ—Ç–æ—á–∏–µ–º: {raw_phones}")
            if view_all_phones.count() > 0:
                logger.debug(
                    f"üîÅ –ü–æ–ø—ã—Ç–∫–∞ #{attempt + 1} ‚Äî –∫–ª–∏–∫–∞–µ–º –ø–æ–≤—Ç–æ—Ä–Ω–æ –ø–æ –∫–Ω–æ–ø–∫–µ —Ç–µ–ª–µ—Ñ–æ–Ω–æ–≤")
                view_all_phones.first.click()

            attempt += 1

        # –¥–∞–∂–µ –µ—Å–ª–∏ –µ—Å—Ç—å "...", —Å–æ—Ö—Ä–∞–Ω—è–µ–º –≤—Å—ë —á—Ç–æ –Ω–∞—à–ª–∏
        if not phones:
            phones = raw_phones

    except Exception as e:
        logger.debug(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–ª—É—á–µ–Ω–∏–∏ —Ç–µ–ª–µ—Ñ–æ–Ω–æ–≤: {e}")

    return address, email, phones, website


def get_socials(wrapper):
    data = {
        'Instagram': [],
        'Telegram': [],
        'WhatsApp': [],
        '–í–ö–æ–Ω—Ç–∞–∫—Ç–µ': [],
        '–û–¥–Ω–æ–∫–ª–∞—Å—Å–Ω–∏–∫–∏': [],
        'Facebook': [],
        'Twitter': [],
        'YouTube': []
    }

    links = wrapper.locator(
        '[aria-label="Instagram"], [aria-label="Telegram"], [aria-label="WhatsApp"],'
        '[aria-label="–í–ö–æ–Ω—Ç–∞–∫—Ç–µ"], [aria-label="–û–¥–Ω–æ–∫–ª–∞—Å—Å–Ω–∏–∫–∏"],'
        '[aria-label="Facebook"], [aria-label="Twitter"], [aria-label="YouTube"]'
    )
    count = links.count()

    for i in range(count):
        link = links.nth(i)
        raw_url = link.get_attribute("href")
        name = link.get_attribute("aria-label")

        if raw_url:
            decoded_url = unquote(raw_url)
            final_data = decode_possible_base64_url(decoded_url)

            if name == "WhatsApp":
                match = re.search(r'wa\.me/(\d+)', final_data)
                value = match.group(1) if match else final_data

            elif name == "Telegram":
                match = re.search(r't\.me/([\w@+]+)', final_data)
                value = f"@{match.group(1)}" if match else final_data

            elif name == "–í–ö–æ–Ω—Ç–∞–∫—Ç–µ":
                match = re.search(r'vk\.com/([\w-]+)', final_data)
                value = match.group(1) if match else final_data

            elif name == "–û–¥–Ω–æ–∫–ª–∞—Å—Å–Ω–∏–∫–∏":
                match = re.search(r'ok\.ru/([\w-]+)', final_data)
                value = match.group(1) if match else final_data

            elif name == "YouTube":
                match = re.search(r'youtube\.com/(@[\w\d_-]+)', final_data)
                value = match.group(1) if match else final_data

            elif name == "Twitter":
                match = re.search(r'twitter\.com/([\w-]+)', final_data)
                value = match.group(1) if match else final_data

            else:
                try:
                    parsed = urlparse(final_data)
                    value = parsed.netloc + parsed.path
                    value = value.replace("www.", "").lstrip('/')
                except Exception:
                    value = final_data

            data[name].append(value.strip())

    return data


def get_pagination_info(page, selector="div._jcreqo >> ._1xhlznaa", max_cards=12):
    try:
        count_cards_text = page.locator(selector).inner_text(timeout=1500)
        count_cards = int(re.search(r"\d+", count_cards_text).group())
        count_pages = (count_cards + max_cards - 1) // max_cards
        return count_cards, count_pages
    except Exception as e:
        logger.warning(f"‚ö† –ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å —á–∏—Å–ª–æ –∫–∞—Ä—Ç–æ—á–µ–∫: {e}")
        return 0, 1  # –±–µ–∑–æ–ø–∞—Å–Ω–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ


def run_parser_for_region(region, search_word, attempt=1):
    logger.info(
        f"üöÄ –ó–∞–ø—É—Å–∫ –ø–∞—Ä—Å–∏–Ω–≥–∞: —Ä–µ–≥–∏–æ–Ω = '{region}', –∫–∞—Ç–µ–≥–æ—Ä–∏—è = '{search_word}'")
    old_data = read_json_data(f"output/{region}/{search_word}.json")
    existing_names = set(x["name"] for x in old_data)
    collect_data = []

    def process_data(existing_data, new_card_data):
        if new_card_data["name"] not in (x["name"] for x in existing_data):
            collect_data.append(new_card_data)
            existing_data.append(new_card_data)

    try:
        with sync_playwright() as p:
            start_time_program = time.time()
            browser = p.chromium.launch(
                headless=True,
                args=["--disable-blink-features=AutomationControlled"],
            )
            context = browser.new_context(
                user_agent="Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/124.0.0.0 Safari/537.36",
                permissions=[],
                viewport={"width": 1280, "height": 800},
                java_script_enabled=True,
                record_video_dir=None,
                bypass_csp=True,  # –í–æ–∑–º–æ–∂–Ω–æ —É–±—Ä–∞—Ç—å
            )
            context.set_default_timeout(1500)
            page = context.new_page()
            page.set_default_timeout(3000)

            page.goto(
                f"https://2gis.{country}/search/{quote(region)}%20{quote(search_word)}",
                wait_until="domcontentloaded",
                timeout=5000
            )

            count_cards, count_pages = get_pagination_info(page)
            logger.info(
                f"üî¢ –ù–∞–π–¥–µ–Ω–æ {count_cards} –∫–∞—Ä—Ç–æ—á–µ–∫, {count_pages} —Å—Ç—Ä–∞–Ω–∏—Ü –≤ {region}")

            for page_index in range(count_pages + 1):
                start_time_page = time.time()
                logger.info(
                    f"–ü–µ—Ä–µ—Ö–æ–¥ –ø–æ —Å—Ç—Ä–∞–Ω–∏—Ü–µ {page_index + 1} –∏–∑ {count_pages + 1}")

                card_blocks = page.locator("._awwm2v > div")
                total = card_blocks.count()

                for i in range(total):
                    try:
                        start_time_card = time.time()
                        card = card_blocks.nth(i)
                        card.scroll_into_view_if_needed()
                        page.wait_for_timeout(150)

                        if not card.is_visible():
                            logger.debug(
                                f"[{i+1}] ‚ùå –≠–ª–µ–º–µ–Ω—Ç –Ω–µ –≤–∏–¥–µ–Ω ‚Äî –ø—Ä–æ–ø—É—Å–∫")
                            continue

                        card.click(timeout=1500)

                        try:
                            page.wait_for_selector("._fjltwx h1", timeout=2000)
                        except:
                            logger.warning(
                                f"[{i+1}] ‚ö† –ö–∞—Ä—Ç–æ—á–∫–∞ –Ω–µ –ø—Ä–æ–≥—Ä—É–∑–∏–ª–∞—Å—å ‚Äî –ø—Ä–æ–ø—É—Å–∫")
                            continue

                        wrapper = page.locator("._fjltwx")
                        if wrapper.count() == 0:
                            logger.warning(
                                f"[{i+1}] ‚ùå –ö–∞—Ä—Ç–æ—á–∫–∞ –ø—É—Å—Ç–∞ ‚Äî –ø—Ä–æ–ø—É—Å–∫")
                            continue

                        preview_name = wrapper.locator(
                            "h1").text_content().strip()

                        if preview_name in existing_names:
                            logger.debug(
                                f"[{i + 1}] ‚è© {preview_name} —É–∂–µ –≤ –±–∞–∑–µ {region}, –ø—Ä–æ–ø—É—Å–∫")
                            continue
                        else:
                            existing_names.add(preview_name)

                        if wrapper.count() == 0:
                            logger.warning(
                                f"[{i + 1}] ‚ùå –ö–∞—Ä—Ç–æ—á–∫–∞ –≤ {region} –Ω–µ –∑–∞–≥—Ä—É–∑–∏–ª–∞—Å—å")
                            continue

                        name, rating, count_reviews = get_header(wrapper)
                        address, email, phones, website = get_data_card(
                            wrapper)
                        socials = get_socials(wrapper)

                        close_button = page.locator('div._k1uvy >> svg')
                        if close_button.count() > 0:
                            close_button.nth(0).click()

                        logger.info(
                            f"[{i + 1}] ‚úÖ {name} –≤ {region} –£—Å–ø–µ—à–Ω–æ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–∞ –∑–∞ {round(time.time() - start_time_card, 2)} —Å–µ–∫")

                        data_card = {
                            "name": clean_invisible(name),
                            "rating": clean_invisible(rating),
                            "count_reviews": clean_invisible(count_reviews),
                            "address": clean_invisible(address) if address else None,
                            "email": clean_invisible(email) if email else None,
                            "phones": [clean_invisible(phone) for phone in phones] if phones else None,
                            "website": clean_invisible(website) if website else None,
                            "socials": {k: v for k, v in socials.items()}
                        }

                        process_data(old_data, data_card)
                    except Exception as e:
                        logger.exception(
                            f"[{i + 1}] ‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ –∫–∞—Ä—Ç–æ—á–∫–∏ –≤ {region}")

                logger.info(
                    f"üìÑ –í {region} —Å—Ç—Ä–∞–Ω–∏—Ü–∞ {page_index + 1}, —Å–æ–±—Ä–∞–Ω–æ: {len(collect_data)}, –≤—Ä–µ–º—è: {round(time.time() - start_time_page, 2)} —Å–µ–∫")

                if page_index != count_pages:
                    next_buttons = page.locator(
                        'div._1x4k6z7 >> ._n5hmn94 >> svg')
                    try:
                        if next_buttons.count() > 1:
                            next_buttons.nth(1).click()
                        elif next_buttons.count() == 1:
                            next_buttons.first.click()
                        else:
                            raise Exception("–ö–Ω–æ–ø–∫–∞ '–¥–∞–ª—å—à–µ' –Ω–µ –Ω–∞–π–¥–µ–Ω–∞")

                        # –£–±–µ–¥–∏–º—Å—è, —á—Ç–æ —Å—Ç—Ä–∞–Ω–∏—Ü–∞ –∏–∑–º–µ–Ω–∏–ª–∞—Å—å
                        # –ú–æ–∂–Ω–æ –ø–æ–¥—Å—Ç—Ä–∞—Ö–æ–≤–∞—Ç—å—Å—è –ø–∞—É–∑–æ–π
                        page.wait_for_timeout(500)
                        logger.info(f"‚û° –ü–µ—Ä–µ—Ö–æ–¥ –Ω–∞ —Å–ª–µ–¥—É—é—â—É—é —Å—Ç—Ä–∞–Ω–∏—Ü—É —É—Å–ø–µ—à–µ–Ω")
                        page_index += 1

                    except Exception as e:
                        logger.warning(
                            f"‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –ø–µ—Ä–µ–π—Ç–∏ –Ω–∞ —Å–ª–µ–¥—É—é—â—É—é —Å—Ç—Ä–∞–Ω–∏—Ü—É –≤ {region}: {e}")
                        logger.info(
                            f"üö´ –û—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º –ø–∞—Ä—Å–∏–Ω–≥ {region} –Ω–∞ —Ç–µ–∫—É—â–µ–π —Å—Ç—Ä–∞–Ω–∏—Ü–µ")
                        break
                else:
                    logger.info(f"‚úÖ –ü–æ—Å–ª–µ–¥–Ω—è—è —Å—Ç—Ä–∞–Ω–∏—Ü–∞ {region} –¥–æ—Å—Ç–∏–≥–Ω—É—Ç–∞")
    except Exception as e:
        logger.error(
            f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–ø—É—Å–∫–µ –±—Ä–∞—É–∑–µ—Ä–∞ –¥–ª—è —Ä–µ–≥–∏–æ–Ω–∞ {region}: {e}")
    finally:
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ç–æ–ª—å–∫–æ –æ–¥–∏–Ω —Ä–∞–∑
        write_json_data(
            old_data, Path("output") / region / f"{search_word}.json")
        logger.info("–ü—Ä–æ–≥—Ä–∞–º–º–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∞")
        logger.info(
            f"–í—Å–µ–≥–æ —Å–æ–±—Ä–∞–Ω–æ: {len(collect_data)}, –≤—Ä–µ–º—è —Ä–∞–±–æ—Ç—ã: {round(time.time() - start_time_program, 2)} —Å–µ–∫")

        if not collect_data:
            if attempt == 1:
                logger.warning(
                    f"üîÅ –ü–æ–≤—Ç–æ—Ä–Ω—ã–π –∑–∞–ø—É—Å–∫ —Ä–µ–≥–∏–æ–Ω–∞ '{region}' ‚Äî –ø–µ—Ä–≤–∞—è –ø–æ–ø—ã—Ç–∫–∞ –≤–µ—Ä–Ω—É–ª–∞ 0 –∫–∞—Ä—Ç–æ—á–µ–∫")
                return run_parser_for_region(region, search_word, attempt=2)
            else:
                logger.warning(
                    f"‚ö† –†–µ–≥–∏–æ–Ω '{region}' –¥–∞–ª 0 –∫–∞—Ä—Ç–æ—á–µ–∫ ‚Äî –¥–∞–∂–µ –ø–æ—Å–ª–µ –ø–æ–≤—Ç–æ—Ä–∞")
                with open("failed_regions.txt", "a", encoding="utf-8") as f:
                    f.write(f"{region}|{search_word}\n")


if __name__ == '__main__':
    num_processes = 3
    # —Å–∫–æ–ª—å–∫–æ —Ä–∞–∑ –æ–±—Ö–æ–¥–∏—Ç—å –≤—Å–µ —Ä–µ–≥–∏–æ–Ω—ã (1 ‚Äî –º–∏–Ω–∏–º—É–º, 2 ‚Äî —Ç–≤–æ–π —Å–ª—É—á–∞–π)
    max_passes = 2

    start_time_all = time.time()  # –æ–±—â–µ–µ –≤—Ä–µ–º—è —Ä–∞–±–æ—Ç—ã

    for current_pass in range(1, max_passes + 1):
        start_time_pass = time.time()  # –≤—Ä–µ–º—è –æ–¥–Ω–æ–≥–æ –ø—Ä–æ—Ö–æ–¥–∞

        logger.info(
            f"üöÄ –ó–∞–ø—É—Å–∫ –æ–±—Ö–æ–¥–∞ #{current_pass} —Å–æ {num_processes} –ø—Ä–æ—Ü–µ—Å—Å–∞–º–∏")

        with Pool(processes=num_processes) as pool:
            results = []
            for args in args_list:
                r = pool.apply_async(run_parser_for_region, args=args)
                results.append(r)

            for i, r in enumerate(results):
                try:
                    r.wait()  # –ñ–¥—ë–º –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è
                    logger.info(f"‚úÖ –û–±—ä–µ–∫—Ç #{i + 1} –∑–∞–≤–µ—Ä—à—ë–Ω")
                except Exception as e:
                    logger.error(f"‚ùå –û—à–∏–±–∫–∞ –≤ –æ–±—ä–µ–∫—Ç–µ #{i + 1}: {e}")

        pass_time = round(time.time() - start_time_pass, 2)
        logger.info(f"üéâ –û–±—Ö–æ–¥ #{current_pass} –∑–∞–≤–µ—Ä—à—ë–Ω –∑–∞ {pass_time} —Å–µ–∫")

    total_time = round(time.time() - start_time_all, 2)
    logger.info(f"üèÅ –í—Å–µ –æ–±—Ö–æ–¥—ã –∑–∞–≤–µ—Ä—à–µ–Ω—ã –∑–∞ {total_time} —Å–µ–∫")
