from playwright.sync_api import sync_playwright
from urllib.parse import quote
import logging
from urllib.parse import unquote, urlparse
import base64
import time
from playwright.sync_api import TimeoutError as PlaywrightTimeoutError
import json
import re


logging.basicConfig(
    level=logging.DEBUG,
    format="%(asctime)s [%(levelname)s] %(message)s",
    datefmt="%Y-%m-%d %H:%M:%S",
    handlers=[
        logging.FileHandler("2GIS.log", mode="a", encoding="utf-8"),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)


search_word = quote("–ø—Ä–æ–∏–∑–≤–æ–¥—Å—Ç–≤–æ —à—Ç–æ—Ä")
search_city = "moscow"


def clean_invisible(text):
    return re.sub(r'\u2012|\u00a0|\u200b|\+7', '', text).strip()


def save_to_json(data, filename="output.json"):
    try:
        with open(filename, 'w', encoding='utf-8') as f:
            json.dump(data, f, ensure_ascii=False, indent=4)
        logger.info(f"–î–∞–Ω–Ω—ã–µ —É—Å–ø–µ—à–Ω–æ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ {filename}")
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–∏ –¥–∞–Ω–Ω—ã—Ö –≤ {filename}: {e}")


def wait_for_count(locator, min_count=1, timeout=3000):
    start_time = time.time()
    while (time.time() - start_time) * 1000 < timeout:
        if locator.count() >= min_count:
            return True
        time.sleep(0.1)
    return False


def decode_possible_base64_url(url):
    """
    –†–∞—Å–ø–æ–∑–Ω–∞—ë—Ç base64-–∑–∞–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ —á–∞—Å—Ç–∏ –≤ —Å—Å—ã–ª–∫–µ –∏ –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç –ø–µ—Ä–≤—É—é —Å—Ç—Ä–æ–∫—É, –Ω–∞—á–∏–Ω–∞—é—â—É—é—Å—è —Å http.
    JSON –∏–≥–Ω–æ—Ä–∏—Ä—É–µ—Ç—Å—è.
    """
    try:
        for part in url.split('/'):
            # –ù–∞—Ö–æ–¥–∏–º base64-–ø–æ–¥–æ–±–Ω—ã–µ —Ñ—Ä–∞–≥–º–µ–Ω—Ç—ã
            if part.startswith(('aHR0', 'ey')) and len(part) >= 16:
                padded = part + '=' * (-len(part) % 4)
                try:
                    decoded = base64.urlsafe_b64decode(padded).decode(
                        'utf-8', errors='replace').strip()
                except Exception:
                    continue

                # –û–±—Ä–∞–±–æ—Ç–∫–∞ \n –∏ –º–Ω–æ–≥–æ—Å—Ç—Ä–æ—á–Ω—ã—Ö base64
                decoded = decoded.replace('\\n', '\n')
                for line in decoded.splitlines():
                    if line.strip().startswith("http"):
                        return line.strip()

                # –ï—Å–ª–∏ —Å—Å—ã–ª–∫–∞ –Ω–µ –Ω–∞–π–¥–µ–Ω–∞, –º–æ–∂–Ω–æ –≤–µ—Ä–Ω—É—Ç—å –≤–µ—Å—å –¥–µ–∫–æ–¥
                return decoded
    except Exception:
        pass

    # –ï—Å–ª–∏ –Ω–∏—á–µ–≥–æ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ ‚Äî –≤–µ—Ä–Ω—É—Ç—å –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã–π url
    return url


def get_header(wrapper):
    name = wrapper.locator("h1").text_content()
    logger.debug(f"–ù–∞–∑–≤–∞–Ω–∏–µ: {name}")

    rating_elem = wrapper.locator("._y10azs")
    rating = rating_elem.inner_text() if rating_elem.count() > 0 else ""
    logger.debug(f"–†–µ–∏ÃÜ—Ç–∏–Ω–≥: {rating if rating else '‚ùå –û—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç'}")

    reviews_elem = wrapper.locator("._jspzdm")
    count_reviews = reviews_elem.text_content(
    ).split()[0] if reviews_elem.count() > 0 else ""
    logger.debug(
        f"–û—Ç–∑—ã–≤—ã: {count_reviews if count_reviews else '‚ùå –û—Ç—Å—É—Ç—Å—Ç–≤—É—é—Ç'}")

    return name, rating, count_reviews


def get_data_card(wrapper):
    address = None
    website = None
    email = None
    phones = []

    # –ê–¥—Ä–µ—Å
    try:
        address_info = wrapper.locator(
            "div._172gbf8 >> ._49kxlr >> ._13eh3hvq >> ._14quei >> ._wrdavn").nth(0)
        address_info.wait_for(state="visible", timeout=5000)
        address = address_info.inner_text()
        logger.debug(f"–î–æ–±–∞–≤–ª–µ–Ω –∞–¥—Ä–µ—Å: {address}")
    except PlaywrightTimeoutError:
        try:
            address_info = wrapper.locator(
                "div._172gbf8 >> ._49kxlr >> ._13eh3hvq >> ._oqoid").nth(0)
            address_info.wait_for(state="visible", timeout=5000)
            address = address_info.inner_text()
            logger.debug(f"–î–æ–±–∞–≤–ª–µ–Ω –∞–¥—Ä–µ—Å (—Ä–µ–∑–µ—Ä–≤–Ω—ã–π —Å–µ–ª–µ–∫—Ç–æ—Ä): {address}")
        except PlaywrightTimeoutError:

            try:
                address_info = wrapper.locator(
                    "div._172gbf8 >> ._49kxlr >> ._oqoid").nth(0)
                address_info.wait_for(state="visible", timeout=5000)
                address = address_info.inner_text()
                logger.debug(f"–î–æ–±–∞–≤–ª–µ–Ω –∞–¥—Ä–µ—Å (—Ä–µ–∑–µ—Ä–≤–Ω—ã–π —Å–µ–ª–µ–∫—Ç–æ—Ä): {address}")
            except Exception as e:
                logger.debug("–ê–¥—Ä–µ—Å ‚ùå –Ω–µ –Ω–∞–π–¥–µ–Ω –ø–æ –æ–±–æ–∏–º —Å–µ–ª–µ–∫—Ç–æ—Ä–∞–º (—Ç–∞–π–º–∞—É—Ç)")
                logger.debug(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–ª—É—á–µ–Ω–∏–∏ –∞–¥—Ä–µ—Å–∞ (—Ä–µ–∑–µ—Ä–≤): {e}")
    except Exception as e:
        logger.debug(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–ª—É—á–µ–Ω–∏–∏ –∞–¥—Ä–µ—Å–∞: {e}")

    # Email
    try:
        email_info = wrapper.locator(
            'div._172gbf8 >> ._49kxlr >> div >> a[href^="mailto:"]')
        if email_info.count() > 0:
            email = email_info.first.inner_text().strip()
            logger.debug(f"–î–æ–±–∞–≤–ª–µ–Ω email: {email}")
        else:
            logger.debug("Email: ‚ùå –ù–µ –Ω–∞–π–¥–µ–Ω")
    except Exception as e:
        logger.debug(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–ª—É—á–µ–Ω–∏–∏ email: {e}")

    # Website
    try:
        website_info = wrapper.locator(
            'div._172gbf8 >> ._49kxlr >> div >> a[href^="https://"]')
        if website_info.count() > 0:
            website = website_info.first.inner_text().strip()
            logger.debug(f"–î–æ–±–∞–≤–ª–µ–Ω —Å–∞–π—Ç: {website}")
        else:
            logger.debug("Website: ‚ùå –ù–µ –Ω–∞–π–¥–µ–Ω")
    except Exception as e:
        logger.debug(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–ª—É—á–µ–Ω–∏–∏ —Å–∞–π—Ç–∞: {e}")

    # –ö–Ω–æ–ø–∫–∞ "–ü–æ–∫–∞–∑–∞—Ç—å –≤—Å–µ —Ç–µ–ª–µ—Ñ–æ–Ω—ã"
    try:
        view_all_phones = wrapper.locator("._1tkj2hw")
        if view_all_phones.count() > 0:
            view_all_phones.first.wait_for(state="visible", timeout=5000)
            view_all_phones.first.click()
            logger.debug("–¢–µ–ª–µ—Ñ–æ–Ω—ã –∑–∞–≥—Ä—É–∂–µ–Ω—ã")
    except Exception as e:
        logger.debug(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –∫–ª–∏–∫–µ –Ω–∞ –∫–Ω–æ–ø–∫—É —Ç–µ–ª–µ—Ñ–æ–Ω–æ–≤: {e}")

    # –¢–µ–ª–µ—Ñ–æ–Ω—ã
    try:
        phones_info = wrapper.locator(
            'div._172gbf8 >> ._49kxlr >> div >> a[href^="tel:"]')
        for i in range(phones_info.count()):
            phone = phones_info.nth(i).inner_text().strip()
            phones.append(phone)
            logger.debug(f"–î–æ–±–∞–≤–ª–µ–Ω —Ç–µ–ª–µ—Ñ–æ–Ω: {phone}")
        if not phones:
            logger.debug("–ù–æ–º–µ—Ä–∞ —Ç–µ–ª–µ—Ñ–æ–Ω–æ–≤: ‚ùå –ù–µ –Ω–∞–π–¥–µ–Ω—ã")
    except Exception as e:
        logger.debug(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–ª—É—á–µ–Ω–∏–∏ —Ç–µ–ª–µ—Ñ–æ–Ω–æ–≤: {e}")

    return address, email, phones, website


def get_socials(wrapper):
    data = {
        'Instagram': [],
        'Telegram': [],
        'WhatsApp': [],
        '–í–ö–æ–Ω—Ç–∞–∫—Ç–µ': [],
        '–û–¥–Ω–æ–∫–ª–∞—Å—Å–Ω–∏–∫–∏': [],
        'Facebook': [],
        'Twitter': [],
        'YouTube': []
    }

    links = wrapper.locator(
        '[aria-label="Instagram"], [aria-label="Telegram"], [aria-label="WhatsApp"],'
        '[aria-label="–í–ö–æ–Ω—Ç–∞–∫—Ç–µ"], [aria-label="–û–¥–Ω–æ–∫–ª–∞—Å—Å–Ω–∏–∫–∏"],'
        '[aria-label="Facebook"], [aria-label="Twitter"], [aria-label="YouTube"]'
    )
    count = links.count()
    logger.debug(f"–ù–∞–π–¥–µ–Ω–æ —Å–æ—Ü–∏–∞–ª—å–Ω—ã—Ö —Å—Å—ã–ª–æ–∫: {count}")

    for i in range(count):
        link = links.nth(i)
        raw_url = link.get_attribute("href")
        name = link.get_attribute("aria-label")

        if raw_url:
            decoded_url = unquote(raw_url)
            final_data = decode_possible_base64_url(decoded_url)

            if name == "WhatsApp":
                match = re.search(r'wa\.me/(\d+)', final_data)
                value = match.group(1) if match else final_data

            elif name == "Telegram":
                match = re.search(r't\.me/([\w@+]+)', final_data)
                value = f"@{match.group(1)}" if match else final_data

            elif name == "–í–ö–æ–Ω—Ç–∞–∫—Ç–µ":
                match = re.search(r'vk\.com/([\w-]+)', final_data)
                value = match.group(1) if match else final_data

            elif name == "–û–¥–Ω–æ–∫–ª–∞—Å—Å–Ω–∏–∫–∏":
                match = re.search(r'ok\.ru/([\w-]+)', final_data)
                value = match.group(1) if match else final_data

            elif name == "YouTube":
                match = re.search(r'youtube\.com/(@[\w\d_-]+)', final_data)
                value = match.group(1) if match else final_data

            elif name == "Twitter":
                match = re.search(r'twitter\.com/([\w-]+)', final_data)
                value = match.group(1) if match else final_data

            else:
                try:
                    parsed = urlparse(final_data)
                    value = parsed.netloc + parsed.path
                    value = value.replace("www.", "").lstrip('/')
                except Exception:
                    value = final_data

            data[name].append(value.strip())
            logger.debug(f"{name}: {value}")

    return data


def get_pagination_info(page, selector="div._jcreqo >> ._1xhlznaa", max_cards=12):
    count_cards_text = page.locator(selector).text_content()
    count_cards = int(count_cards_text)
    count_pages = count_cards // max_cards
    last_page_count_cards = count_cards % max_cards

    logger.debug(
        f"–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å—Ç—Ä–∞–Ω–∏—Ü: {count_pages + 1}, –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –∫–∞—Ä—Ç–æ—á–µ–∫ –Ω–∞ –ø–æ—Å–ª–µ–¥–Ω–µ–π —Å—Ç—Ä–∞–Ω–∏—Ü–µ: {last_page_count_cards}")
    logger.info(f"–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –∫–∞—Ä—Ç–æ—á–µ–∫: {count_cards}")

    return count_cards, count_pages, last_page_count_cards


collect_data = []
try:
    with sync_playwright() as p:
        start_time_program = time.time()
        browser = p.chromium.launch(headless=False)
        context = browser.new_context()
        page = context.new_page()

        logging.debug("–ë—Ä–∞—É–∑–µ—Ä –∑–∞–ø—É—â–µ–Ω")
        page.goto(f"https://2gis.kz/{search_city}/search/{search_word}")
        logging.debug("–û—Ç–∫—Ä—ã—Ç–∞ —Å—Ç—Ä–∞–Ω–∏—Ü–∞ –ø–æ–∏—Å–∫–∞")

        count_cards, count_pages, last_page_count_cards = get_pagination_info(
            page)

        for page_index in range(count_pages + 1):
            start_time_page = time.time()
            logger.info(
                f"–ü–µ—Ä–µ—Ö–æ–¥ –ø–æ —Å—Ç—Ä–∞–Ω–∏—Ü–µ {page_index + 1} –∏–∑ {count_pages + 1}")
            items = page.locator("._1kf6gff")
            logger.debug(f"–ü–æ–ª—É—á–µ–Ω–∏–µ –æ–±—â–∏—Ö –¥–∞–Ω–Ω—ã—Ö")

            try:
                for i in range(12 if page_index != count_pages else last_page_count_cards):
                    start_time_card = time.time()
                    item = items.nth(i)
                    address = None
                    website = None
                    email = None
                    phones = []

                    logger.info(f"[{i + 1}] –û–±—Ä–∞–±–æ—Ç–∫–∞ –∫–∞—Ä—Ç–æ—á–∫–∏")
                    if item.count() > 0:
                        item.wait_for(state="visible")
                        item.click()
                    wrapper = page.locator("._fjltwx")
                    wrapper.first.wait_for(
                        state="visible", timeout=7000)

                    name, rating, count_reviews = get_header(wrapper)
                    address, email, phones, website = get_data_card(
                        wrapper)
                    socials = get_socials(wrapper)

                    page.locator('div._k1uvy >> svg').nth(
                        0).click()  # –ó–∞–∫—Ä—ã—Ç–∏–µ –∫–∞—Ä—Ç–æ—á–∫–∏

                    logger.info(
                        f"[{i+1}] ‚úÖ –£—Å–ø–µ—à–Ω–æ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–∞ –∑–∞ {round(time.time() - start_time_card, 2)} —Å–µ–∫")

                    collect_data.append({
                        "name": clean_invisible(name),
                        "rating": clean_invisible(rating),
                        "count_reviews": clean_invisible(count_reviews),
                        "address": clean_invisible(address) if address else None,
                        "email": clean_invisible(email) if email else None,
                        "phones": [clean_invisible(phone) for phone in phones] if phones else None,
                        "website": clean_invisible(website) if website else None,
                        "socials": {k: v for k, v in socials.items()}
                    })

            except Exception as e:
                logger.exception(
                    f"[{i + 1}] ‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ –∫–∞—Ä—Ç–æ—á–∫–∏")
            finally:
                logger.info(
                    f"üìÑ –°—Ç—Ä–∞–Ω–∏—Ü–∞ {page_index}, –≤—Å–µ–≥–æ —Å–æ–±—Ä–∞–Ω–æ: {len(collect_data)}, ‚úÖ –£—Å–ø–µ—à–Ω–æ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–∞ –∑–∞ {round(time.time() - start_time_page, 2)} —Å–µ–∫")
                logger.debug(
                    f"–°—Ç—Ä–∞–Ω–∏—Ü–∞: {page_index}, –û—Å—Ç–∞–ª–æ—Å—å: {count_pages}")
                if page_index != count_pages:
                    next_buttons = page.locator(
                        'div._1x4k6z7 >> ._n5hmn94 >> svg')
                    if page_index == 0:
                        next_buttons.nth(0).click()
                    else:
                        next_buttons.nth(1).click()
                    logger.debug(
                        f"–ù–∞–∂–∞—Ç–∏–µ –Ω–∞ –∫–Ω–æ–ø–∫—É –ø–µ—Ä–µ—Ö–æ–¥–∞ –Ω–∞ —Å–ª–µ–¥—É—é—â—É—é —Å—Ç—Ä–∞–Ω–∏—Ü—É")

                else:
                    logger.info("–î–æ—à–ª–∏ –¥–æ –ø–æ—Å–ª–µ–¥–Ω–µ–π —Å—Ç—Ä–∞–Ω–∏—Ü—ã, –∑–∞–≤–µ—Ä—à–∞–µ–º —Ü–∏–∫–ª")
                    logger.info("‚úÖ –¶–∏–∫–ª –∑–∞–≤–µ—Ä—à–µ–Ω")
                    break
except Exception as e:
    logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–ø—É—Å–∫–µ –±—Ä–∞—É–∑–µ—Ä–∞: {e}")
finally:
    logger.info("–ü—Ä–æ–≥—Ä–∞–º–º–∞ –ø—Ä–µ–∫—Ä–∞—â–µ–Ω–∞")
    save_to_json(collect_data, "output.json")
    logger.info(
        f"–í—Å–µ–≥–æ —Å–æ–±—Ä–∞–Ω–æ: {len(collect_data)}, –í—ã–ø–æ–ª–Ω–µ–Ω–∞ –∑–∞ {round(time.time() - start_time_program, 2)} —Å–µ–∫")
